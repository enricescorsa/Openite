---
title: Pdf to text
author: Enric Escorsa
date: '2020-03-29'
slug: pdf-to-text
categories:
  - experiments
tags:
  - ubi
  - pdf
  - text
description: Desc
hacker_news_id: ''
lobsters_id: ''
meta_img: /images/image.jpg
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Em proposo analitzar el contingut d'un arxiu PDF. L'arxiu que llegiré amb R és un informe recent del Banc Mundial sobre la Renda Bàsica Universal ( _Universal Basic Income_ ) coordinat per Ugo Gentilini (@Ugentilini) anomenat "Exploring Universal Basic Income" disponible [aquí](http://documents.worldbank.org/curated/en/993911574784667955/pdf/Exploring-Universal-Basic-Income-A-Guide-to-Navigating-Concepts-Evidence-and-Practices.pdf). 
Vull basar-me en [aquest enfocament](https://www.garrickadenbuie.com/blog/redacted-text-extracted-mueller-report/) de Garrick Aden-Buie (@grrrck) per extraure'n i representar-ne el contingut.

```{r, warning=FALSE, message=FALSE}
#primer carrego paquets necessaris
library(tidyverse)
library(ggpage)
library(tidytext)
library(stringr)
library(pdftools)

#llegeixo pdf ja baixat de la URL (http://documents.worldbank.org/curated/en/993911574784667955/pdf/Exploring-Universal-Basic-Income-A-Guide-to-Navigating-Concepts-Evidence-and-Practices.pdf) al meu directori local i amb el paquet pdftools el transformo en un csv per a que estigui en format tidytext
meu_text <- pdftools::pdf_text("Exploring-Universal-Basic-Income-A-Guide-to-Navigating-Concepts-Evidence-and-Practices.pdf")

meu_report <- tibble(
  page = 1:length(meu_text),
  text = meu_text
) %>% 
  separate_rows(text, sep = "\n") %>% 
  group_by(page) %>% 
  mutate(line = row_number()) %>% 
  ungroup() %>% 
  select(page, line, text)

#les 25 primeres pagines son palla introductoria, presentacions, index, etc. Les suprimeixo
meu_report <- meu_report %>% filter(page %in% (26:337))

```


Un cop llegit puc fer un recompte de les paraules més mencionades

```{r, warning=FALSE, message=FALSE}
report_net <- meu_report %>%
  unnest_tokens(word, text)

# eliminar stop words
data(stop_words)

#afegeixo algunes paraules més que veig que no m'interessen als stopwords (noms dels autors de l'informe i altres)
library(tm)
messtopwords <- tibble(word =c(as.character(1:10),"www.palgrave.com","palgrave","http","https", "documents.worldbank.org", "bibliography", "pdf", "ugo", "gentilini", "margaret", "grosh", "jamele", "rigolini", "ruslan", "yemtsov", "doi.org", "0.0", "0.7", "bit.ly"), lexicon="SMART")

#uneixo dues taules de stopwords
meus_stop_words <- bind_rows(stop_words, messtopwords)
                                    
report_net <- report_net %>%
  anti_join(meus_stop_words)

recompte_paraules <- report_net %>%
  count(word, sort = TRUE)

recompte_paraules
```

Els Països que apareixen més mencionats en aquest informe son: India (140), Brasil (66), Regne Unit (57), Indonesia (51), Russia (44), Chile (42), Mozambique i Finlandia (38).

Puc fer un núvol de paraules per representar les paraules més freqüents

```{r, warning=FALSE, message=FALSE}
library(wordcloud)
wordcloud(words = recompte_paraules$word, freq = recompte_paraules$n, min.freq = 1,
          max.words=125, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(4, "Set1"))
```

Estaria bé també veure si apareixen alguns conceptes que m'interessen com per exemple: "Intel·ligència Artificial", "Gig-economy", "zero-hour contracts", o "auto-ocupació",... i sobretot veure amb quins altres conceptes estan correlacionats.


La funció _pairwise_cor()_ del paquet _widyr_ ens permet trobar les principals correlacions de paraules en base a si apareixen sovint en la mateixa secció del document.

```{r, warning=FALSE, message=FALSE}
library(widyr)
#primer extraiem les paraules del text
unnested <- meu_report %>%
        unnest_tokens(word, text) %>%
        filter(!word %in% meus_stop_words$word)

#després usem la funció pairwise_cor per obtenir correlació a cada pàgina
word_cor <- unnested %>%
   group_by(word) %>%
   filter(n() >= 15) %>%
   pairwise_cor(word, page)
```

Amb quines paraules està més correlacionada, per exemple, la paraula "economy"

```{r, warning=FALSE, message=FALSE}
word_cor %>%
        arrange(desc(correlation)) %>% filter(item1 == "economy")
```

I la paraula "employment"?

```{r, warning=FALSE, message=FALSE}
word_cor %>%
        arrange(desc(correlation)) %>% filter(item1 == "employment")
```

Podem representar totes aquestes relacions (vincles entre les paraules més correlacionades)

```{r, warning=FALSE, message=FALSE}
library(igraph)
library(ggraph)
word_cor %>%
  filter(!is.na(correlation),
         correlation > .65) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "#EE8C00", size = 2) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()
```


Finalment, per veure en quines pàgines de l'informe es mencionen paraules que m'interessen podria generar una representació visual del document.

```{r, warning=FALSE, message=FALSE}

pagines <- 
  meu_report %>% 
  # pagines sense apenes text
  complete(
    page, 
    line = 1:max(meu_report$line),
    fill = list(text = "")
  ) %>% 
  # Pre-processament amb ggpage
  ggpage_build(
    ncol = 20, 
    bycol = FALSE, 
    page.col = "page", 
    wtl = FALSE, 
    x_space_pages = 10,
    y_space_pages = 100
  ) %>% 
  mutate(
    color = case_when(
      str_detect(word, "benefit|benefits") ~ "beneficis",
      str_detect(word, "impact|effects|evidence") ~ "evidència",
      str_detect(word, "gig|gig-economy") ~ "gig-economy",
      str_detect(word, "zero-hour|zero-hour contract|zero hour contracts|zero-hour contracts") ~ "zero-hour contracts",
      str_detect(word, "artificial|AI")  ~ "AI",    
      TRUE ~ "normal"
    ),
    color = factor(color, c("beneficis", "evidència", "gig-economy", "zero-hour contracts", "AI", "normal"))
  )

#fixar paleta de colors de ggthemes::pal_calc()
colors <- rep("", length(levels(pagines$color)))
names(colors) <- levels(pagines$color)
colors["beneficis"] <- "#DA121A"
colors["evidència"] <- "#0F47AF"
colors["gig-economy"] <- "#FE4722"
colors["zero-hour contracts"] <- "#F6A414"
colors["AI"] <- "#FFFF00"
colors["normal"] <- "#d0d0d0"


#finalment uso ggpage_plot() de ggpage per crear una visualitzacio de les pagines del report
ggpage_plot(pagines) +
  aes(fill = color) +
  scale_fill_manual(
    values = colors, 
    breaks = setdiff(names(colors), "normal")
  ) +
  labs(fill = NULL, caption = "basat in @grrrck") +
  guides(fill = guide_legend(nrow = 1)) +
  theme(legend.position = "top")

```

No es menciona a l'informe ni la gig-economy, ni els contractes zero hores, i només molt esporàdicament la Intel·ligència Artificial...
